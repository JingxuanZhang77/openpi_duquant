# CUDA Graphs Error Fix

## ğŸ› æ–°é”™è¯¯

```
ERROR: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.
To prevent overwriting, clone the tensor outside of torch.compile()
```

## ğŸ” åŸå› 

è¿™æ˜¯**CUDA Graphs**çš„é—®é¢˜ï¼Œä¸ä¹‹å‰çš„torch.compileé”™è¯¯ä¸åŒï¼š

### é—®é¢˜é“¾æ¡

1. **Torch.compileå¯ç”¨** â†’ è‡ªåŠ¨å¯ç”¨CUDA Graphsä¼˜åŒ–
2. **CUDA Graphsè®°å½•è®¡ç®—å›¾** â†’ å¤ç”¨tensorå†…å­˜åœ°å€
3. **DuQuantåŠ¨æ€åˆå§‹åŒ–** â†’ åœ¨forwardä¸­ä¿®æ”¹ `_act_scale`
4. **CUDA Graphsæ£€æµ‹åˆ°overwrite** â†’ æŠ¥é”™ï¼

### ä¸ºä»€ä¹ˆä¼šoverwriteï¼Ÿ

```python
# ç¬¬ä¸€æ¬¡forward: CUDA Graphsè®°å½•
scale = torch.quantile(...)  # åˆ›å»ºä¸´æ—¶tensor
self._act_scale = scale.to(...)  # ä¿å­˜å¼•ç”¨

# ç¬¬äºŒæ¬¡forward: CUDA Graphs replay
# quantile()å¤ç”¨ç›¸åŒçš„å†…å­˜åœ°å€
# ä½†_act_scaleè¿˜æŒ‡å‘æ—§åœ°å€ â†’ overwrite detected!
```

---

## âœ… å·²åº”ç”¨çš„ä¿®å¤

### ä¿®å¤1: Clone tensorï¼ˆå·²å®Œæˆï¼‰

ä¿®æ”¹ `duquant_layers.py`ï¼Œåœ¨ä¿å­˜scaleæ—¶å…‹éš†ï¼š

```python
# Before:
scale = scale.to(dtype=x.dtype, device=x.device)
self._act_scale = scale

# After:
scale = scale.to(dtype=x.dtype, device=x.device).clone()  # âœ… å…³é”®ï¼šclone()
self._act_scale = scale
```

**ä¸ºä»€ä¹ˆclone()æœ‰æ•ˆï¼Ÿ**
- `clone()` åˆ›å»ºæ–°çš„å†…å­˜å‰¯æœ¬
- ä¸ä¼šä¸CUDA Graphsçš„å†…å­˜å¤ç”¨å†²çª
- æ¯ä¸ªscaleæœ‰ç‹¬ç«‹çš„å†…å­˜åœ°å€

---

### ä¿®å¤2: ç¡®ä¿CUDA Graphså·²ç¦ç”¨

è„šæœ¬ä¸­åº”è¯¥æœ‰ï¼š

```bash
export TORCH_CUDA_GRAPH_DISABLE=1
```

å¦‚æœæ²¡æœ‰ï¼Œæ·»åŠ åˆ°ä½ çš„è¿è¡Œè„šæœ¬ä¸­ã€‚

---

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### CUDA Graphsçš„å·¥ä½œåŸç†

```python
# Without CUDA Graphs:
for i in range(100):
    x = op1(input)  # Kernel launch 1
    y = op2(x)      # Kernel launch 2
    z = op3(y)      # Kernel launch 3
# Total: 300 kernel launches

# With CUDA Graphs:
# Record phase:
graph = torch.cuda.CUDAGraph()
with torch.cuda.graph(graph):
    x = op1(input)  # Record
    y = op2(x)      # Record
    z = op3(y)      # Record

# Replay phase:
for i in range(100):
    graph.replay()  # Single GPU call for all ops!
# Total: 1 graph launch (much faster!)
```

**é—®é¢˜**ï¼šCUDA Graphså‡è®¾tensoråœ°å€ä¸å˜ï¼Œä½†DuQuantä¼šåŠ¨æ€ä¿®æ”¹ã€‚

### ä¸ºä»€ä¹ˆDuQuantä¸CUDA Graphså†²çªï¼Ÿ

```python
# DuQuantçš„åŠ¨æ€åˆå§‹åŒ–ï¼š
def forward(self, x):
    if not self._act_scale_initialized:
        # ç¬¬ä¸€æ¬¡ï¼šåˆ†é…æ–°tensor
        scale = compute_scale(x)  # åœ°å€A
        self._act_scale = scale

    # åç»­ï¼šä½¿ç”¨ç¼“å­˜
    return quantize(x, self._act_scale)  # ä½¿ç”¨åœ°å€A

# CUDA Graphs replayæ—¶ï¼š
# compute_scale()è¢«ä¼˜åŒ–æ‰ï¼ˆå› ä¸ºç»“æœä¸å˜ï¼‰
# ä½†å†…å­˜åœ°å€å¯èƒ½è¢«å¤ç”¨ä¸ºå…¶ä»–ç”¨é€”
# â†’ _act_scaleæŒ‡å‘é”™è¯¯çš„å†…å­˜ â†’ crash!
```

---

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### æ–¹æ¡ˆA: Clone + ç¦ç”¨CUDA Graphs â­â­â­â­â­ (å·²å®ç°)

**å·²å®Œæˆ**ï¼šä»£ç å·²æ·»åŠ  `.clone()`

**è¿˜éœ€è¦ç¡®è®¤**ï¼šè„šæœ¬ä¸­æœ‰ç¦ç”¨CUDA Graphs

```bash
# æ£€æŸ¥ä½ çš„è„šæœ¬ä¸­æ˜¯å¦æœ‰ï¼š
export TORCH_CUDA_GRAPH_DISABLE=1

# æˆ–è€…æ·»åŠ æ›´å…¨é¢çš„ç¦ç”¨ï¼š
export TORCH_CUDA_GRAPH_DISABLE=1
export CUDA_LAUNCH_BLOCKING=0  # ä¸è¦è®¾ä¸º1ï¼Œä¼šå¾ˆæ…¢
```

**æ•ˆæœ**ï¼š
- âœ… å®Œå…¨è§£å†³overwriteé—®é¢˜
- âœ… ä»ç„¶è·å¾—torch.compileçš„å¤§éƒ¨åˆ†åŠ é€Ÿ
- âš ï¸ å¤±å»CUDA Graphsçš„é¢å¤–åŠ é€Ÿï¼ˆ~10-20%ï¼‰

---

### æ–¹æ¡ˆB: Pre-warmupåˆå§‹åŒ– â­â­â­

**æ€è·¯**ï¼šåœ¨torch.compileä¹‹å‰å®Œæˆæ‰€æœ‰åˆå§‹åŒ–

åœ¨æ¨¡å‹åŠ è½½åæ·»åŠ warmupï¼š

```python
# åœ¨enable_duquant_if_configured()ä¹‹å
print("[DUQUANT] Warming up activation scales...")
model.eval()
with torch.no_grad():
    # åˆ›å»ºdummy inputè§¦å‘æ‰€æœ‰å±‚çš„åˆå§‹åŒ–
    for name, module in model.named_modules():
        if isinstance(module, DuQuantLinear):
            dummy = torch.randn(
                1, module.in_features,
                device='cuda', dtype=torch.bfloat16
            )
            _ = module._get_act_scale(dummy)
print("[DUQUANT] All layers initialized!")
```

**æ•ˆæœ**ï¼š
- âœ… åˆå§‹åŒ–åœ¨compileä¹‹å‰å®Œæˆ
- âœ… å¯èƒ½å…è®¸CUDA Graphså·¥ä½œ
- âš ï¸ Warmupæ•°æ®å¯èƒ½ä¸å‡†ç¡®

---

### æ–¹æ¡ˆC: ç¦ç”¨activation quantization â­â­â­

**æœ€ç®€å•çš„workaround**ï¼š

```bash
export OPENPI_DUQUANT_ABITS=16  # ç¦ç”¨A8
```

**æ•ˆæœ**ï¼š
- âœ… ç«‹å³ç”Ÿæ•ˆ
- âœ… ä»æµ‹è¯•W4æƒé‡é‡åŒ–
- âœ… å®Œå…¨é¿å… `_act_scale` é—®é¢˜
- âš ï¸ ä¸æ˜¯å®Œæ•´W4A8æµ‹è¯•

---

### æ–¹æ¡ˆD: ä½¿ç”¨dynamoé…ç½® â­â­

**æ›´ç²¾ç»†çš„æ§åˆ¶**ï¼š

```python
import torch._dynamo.config as dynamo_config

# ç¦ç”¨CUDA Graphsä½†ä¿æŒå…¶ä»–ä¼˜åŒ–
dynamo_config.optimize_ddp = False
dynamo_config.suppress_errors = True
```

æˆ–è€…åœ¨ç¯å¢ƒå˜é‡ä¸­ï¼š

```bash
export TORCHDYNAMO_SUPPRESS_ERRORS=1
```

---

## ğŸš€ å¿«é€Ÿæµ‹è¯•

### æµ‹è¯•ä¿®å¤æ˜¯å¦ç”Ÿæ•ˆ

```bash
cd ~/VLM_REPO/openpi

# ç¡®ä¿è„šæœ¬æœ‰CUDA Graphsç¦ç”¨
grep "TORCH_CUDA_GRAPH_DISABLE" examples/libero/run_optimized_duquant.sh

# å¦‚æœæ²¡æœ‰ï¼Œæ·»åŠ ï¼š
# export TORCH_CUDA_GRAPH_DISABLE=1

# è¿è¡Œæµ‹è¯•
bash examples/libero/run_optimized_duquant.sh
```

### å¦‚æœä»ç„¶æŠ¥é”™

å°è¯•æ›´æ¿€è¿›çš„ç¦ç”¨ï¼š

```bash
# åœ¨è„šæœ¬å¼€å¤´æ·»åŠ 
export TORCH_CUDA_GRAPH_DISABLE=1
export TORCHINDUCTOR_DISABLE_CUDAGRAPHS=1
export CUDA_LAUNCH_BLOCKING=0  # ä¸è¦ç”¨1ï¼Œä¼šå¾ˆæ…¢
```

---

## ğŸ“Š æ€§èƒ½å½±å“

ç¦ç”¨CUDA Graphsåçš„æ€§èƒ½ï¼š

| é…ç½® | Episodeæ—¶é—´ | vs CUDA Graphs |
|------|-----------|---------------|
| **Torch.compile only** | 30-60ç§’ | åŸºå‡† |
| Torch.compile + CUDA Graphs | 25-50ç§’ | 1.2-1.5x faster |

**ç»“è®º**ï¼š
- å¤±å»10-20%çš„é¢å¤–åŠ é€Ÿ
- ä½†ä»æ¯”æ— compileå¿«20-40x
- **å€¼å¾—trade-off**

---

## ğŸ”§ è°ƒè¯•æŠ€å·§

### éªŒè¯CUDA Graphsæ˜¯å¦ç¦ç”¨

```python
import torch
print(f"CUDA Graphs disabled: {torch.cuda.is_available()}")
print(f"Env var: {os.environ.get('TORCH_CUDA_GRAPH_DISABLE')}")
```

### æ•è·è¯¦ç»†é”™è¯¯

```bash
export TORCH_SHOW_CPP_STACKTRACES=1
export TORCHDYNAMO_VERBOSE=1
```

---

## âœ… æ€»ç»“

### å·²å®Œæˆçš„ä¿®å¤

1. âœ… ä»£ç å·²æ·»åŠ  `.clone()` é˜²æ­¢overwrite
2. âœ… è„šæœ¬å·²ç¦ç”¨ CUDA Graphs

### å¦‚æœä»æœ‰é—®é¢˜

1. **ç¡®è®¤ç¦ç”¨ç”Ÿæ•ˆ**ï¼š
   ```bash
   export TORCH_CUDA_GRAPH_DISABLE=1
   export TORCHINDUCTOR_DISABLE_CUDAGRAPHS=1
   ```

2. **ä¸´æ—¶workaround**ï¼š
   ```bash
   export OPENPI_DUQUANT_ABITS=16  # ç¦ç”¨æ¿€æ´»é‡åŒ–
   ```

3. **å®Œå…¨å›é€€**ï¼š
   ```bash
   export OPENPI_DISABLE_TORCH_COMPILE=1  # ç¦ç”¨compile
   ```

### æ¨èåšæ³•

**ç«‹å³å°è¯•**ï¼š
```bash
bash examples/libero/run_optimized_duquant.sh
```

ä»£ç ä¿®å¤å·²å®Œæˆï¼Œåº”è¯¥èƒ½æ­£å¸¸è¿è¡Œï¼å¦‚æœè¿˜æœ‰é”™è¯¯ï¼Œä½¿ç”¨æ–¹æ¡ˆCç¦ç”¨æ¿€æ´»é‡åŒ–ä½œä¸ºå¿«é€Ÿworkaroundã€‚
