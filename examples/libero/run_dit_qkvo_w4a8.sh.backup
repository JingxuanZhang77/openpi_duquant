#!/bin/bash
# DuQuant W4A8 for DiT QKVO layers only (90 layers)
# Quantizes only attention layers (q_proj, k_proj, v_proj, o_proj) in the DiT
# Excludes MLP layers (gate_proj, up_proj, down_proj) for comparison
#
# Model Structure:
# ┌────────────────────────────────────────────────────────────┐
# │ OpenPI Model (PI0Pytorch)                                  │
# │                                                            │
# │ ├── paligemma_with_expert.paligemma                        │
# │ │   ├── vision_tower (SigLIP - NOT quantized)             │
# │ │   └── language_model (Gemma LLM - NOT quantized)        │
# │ │                                                          │
# │ └── paligemma_with_expert.gemma_expert (DiT - 18 layers)  │
# │     └── model                                              │
# │         ├── layers[0..17].self_attn.{q,k,v,o}_proj ← QUANTIZE (18×4 = 72)
# │         └── layers[0..17].self_attn_2.{q,k,v,o}_proj ← QUANTIZE (18×4 = 72, if exists)
# │                                                            │
# └────────────────────────────────────────────────────────────┘
#
# Expected layer count:
#   - If single attention: 18 layers × 4 projections = 72 layers
#   - If dual attention: 18 layers × 4 projections × 2 = 144 layers
#   - Actual count depends on DiT architecture (check dry-run output)

set -e

cd ~/VLM_REPO/openpi
source examples/libero/.venv/bin/activate
export CKPT=~/VLM_REPO/openpi/ckpts/pi05_libero_torch

# Check CKPT
if [ -z "$CKPT" ]; then
    echo "Error: CKPT environment variable must be set"
    echo "Usage: export CKPT=/path/to/checkpoint"
    exit 1
fi

# Set environment
export PYTHONPATH=$PWD/src:$PWD/third_party/libero

# ============================================
# DiT QKVO W4A8 DuQuant Configuration
# ============================================
# Target: Only attention layers (q_proj, k_proj, v_proj, o_proj)
# Exclude: MLP layers (gate_proj, up_proj, down_proj)
# ============================================
export OPENPI_DUQUANT_DEBUG=1
export OPENPI_DUQUANT_SCOPE="paligemma_with_expert.gemma_expert.model."
export OPENPI_DUQUANT_WBITS_DEFAULT=4
export OPENPI_DUQUANT_ABITS=8
export OPENPI_DUQUANT_BLOCK=16
export OPENPI_DUQUANT_PERMUTE=1           # Enable input permutation
export OPENPI_DUQUANT_ROW_ROT=restore     # Enable rotation with output restoration
export OPENPI_DUQUANT_ACT_PCT=99.9
export OPENPI_DUQUANT_CALIB_STEPS=32      # Calibration steps for activation quantization
export OPENPI_DUQUANT_LS=0.15             # Lambda smooth (only used when PERMUTE=1)

# CRITICAL: Filter to only QKVO layers
# Regex matches: q_proj, k_proj, v_proj, o_proj (NOT gate_proj, up_proj, down_proj)
export OPENPI_DUQUANT_INCLUDE='.*(q_proj|k_proj|v_proj|o_proj).*'
export OPENPI_DUQUANT_EXCLUDE='(?:^|\.)(norm|ln|layernorm|emb|gate_proj|up_proj|down_proj)(?:\.|$)'

# Disable torch.compile for faster startup
export OPENPI_DISABLE_TORCH_COMPILE=1
export TORCH_COMPILE_DISABLE=1
export TORCHDYNAMO_DISABLE=1
unset CUBLAS_WORKSPACE_CONFIG

# Pack directory for DiT QKVO quantization
export OPENPI_DUQUANT_PACKDIR="/home/jz97/VLM_REPO/openpi/duquant_packed_dit_qkvo_w4a8"

# Enable profiling to measure fake quantization overhead
export OPENPI_DUQUANT_PROFILE=1

# Default parameters
TASK_SUITE="${TASK_SUITE:-libero_spatial}"
NUM_TRIALS="${NUM_TRIALS:-20}"
SEED="${SEED:-42}"

echo "========================================"
echo "LIBERO Headless Evaluation"
echo "DuQuant: DiT QKVO Only (W4A8)"
echo "========================================"
echo "Checkpoint: $CKPT"
echo "Task suite: $TASK_SUITE"
echo "Num trials: $NUM_TRIALS"
echo "Seed: $SEED"
echo ""
echo "DuQuant Config (DiT QKVO W4A8):"
echo "  TARGET: DiT Attention layers ONLY (q/k/v/o_proj)"
echo "  SCOPE: $OPENPI_DUQUANT_SCOPE"
echo "  INCLUDE: $OPENPI_DUQUANT_INCLUDE"
echo "  EXCLUDE: $OPENPI_DUQUANT_EXCLUDE"
echo "  WBITS=$OPENPI_DUQUANT_WBITS_DEFAULT"
echo "  ABITS=$OPENPI_DUQUANT_ABITS"
echo "  BLOCK=$OPENPI_DUQUANT_BLOCK"
echo "  PERMUTE=$OPENPI_DUQUANT_PERMUTE"
echo "  ROW_ROT=$OPENPI_DUQUANT_ROW_ROT"
echo "  ACT_PCT=$OPENPI_DUQUANT_ACT_PCT"
echo "  CALIB_STEPS=$OPENPI_DUQUANT_CALIB_STEPS"
echo "  LS=$OPENPI_DUQUANT_LS"
echo "  PACKDIR=$OPENPI_DUQUANT_PACKDIR"
echo ""
echo "⚡ QUANTIZATION TARGET:"
echo "  ✅ DiT Attention (QKVO) - Expected: 72-144 layers"
echo "  ❌ DiT MLP (gate/up/down) - NOT quantized"
echo "  ❌ LLM (Gemma) - NOT quantized"
echo "  ❌ Vision Tower (SigLIP) - NOT quantized"
echo ""
echo "⚡ FEATURES:"
echo "  ✅ W4A8 fake quantization"
echo "  ✅ Input permutation enabled (better accuracy)"
echo "  ✅ Row rotation with output restoration (better accuracy)"
echo "  ✅ Pre-cached rotation matrices (optimized)"
echo "  ✅ Pre-quantized weights (optimized)"
echo "  ✅ Profiling enabled (measure overhead)"
echo ""
echo "Expected layers (check [DUQUANT] Total layers replaced):"
echo "  - Single attention: ~72 layers (18 DiT blocks × 4 QKVO)"
echo "  - Dual attention: ~144 layers (18 DiT blocks × 4 QKVO × 2)"
echo ""
if [ "$OPENPI_DISABLE_TORCH_COMPILE" = "1" ]; then
    echo "  ❌ torch.compile DISABLED (faster startup, slower per-episode)"
    echo ""
    echo "Expected speed:"
    echo "  All episodes: ~2-3 min each"
else
    echo "  ✅ torch.compile ENABLED (CUDA kernel fusion)"
    echo ""
    echo "Expected speed:"
    echo "  Episode 1: ~15-20 min (torch.compile compilation)"
    echo "  Episode 2+: ~30-60s each (using cached compilation)"
fi

echo ""
echo "NOTE: Compare with run_optimized_duquant.sh to measure"
echo "      the contribution of MLP layers to accuracy."
echo "========================================"
echo ""

# Run evaluation with timing
time python examples/libero/main.py \
  --args.headless \
  --args.policy-config pi05_libero \
  --args.policy-dir "$CKPT" \
  --args.task-suite-name "$TASK_SUITE" \
  --args.num-trials-per-task 20 \
  --args.seed "$SEED"

echo ""
echo "========================================"
echo "Evaluation complete!"
echo "Check results in: results/libero/"
echo "Check profiling in stdout for [DUQUANT][PROFILE]"
echo "========================================"
