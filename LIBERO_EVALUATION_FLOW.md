# LIBERO è¯„ä¼°æµç¨‹è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ OpenPI åœ¨ LIBERO ä»»åŠ¡ä¸Šçš„å®Œæ•´è¯„ä¼°æµç¨‹ï¼Œä»è„šæœ¬å¯åŠ¨åˆ°æ¨¡å‹æ¨ç†çš„æ¯ä¸€ä¸ªæ­¥éª¤ã€‚

## 1. æ•´ä½“æµç¨‹æ¦‚è§ˆ

```
bash run_llm_dit_mlp_w4a8.sh
    â†“
examples/libero/main.py (å…¥å£)
    â†“
eval_libero() å‡½æ•°
    â†“
åˆ›å»º Policy å¯¹è±¡ (åŒ…å«æ¨¡å‹ + transforms)
    â†“
LIBERO ç¯å¢ƒå¾ªç¯ (æ¯ä¸ªä»»åŠ¡ Ã— æ¯ä¸ªepisode)
    â†“
Policy.infer() æ¨ç†
    â†“
ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œ
```

## 2. è¯¦ç»†è°ƒç”¨é“¾

### é˜¶æ®µ 1: è„šæœ¬å¯åŠ¨ (run_llm_dit_mlp_w4a8.sh)

**æ–‡ä»¶**: `examples/libero/run_llm_dit_mlp_w4a8.sh`

**å…³é”®æ­¥éª¤**:
1. è®¾ç½®ç¯å¢ƒå˜é‡ (PYTHONPATH, CUDAé…ç½®ç­‰)
2. è®¾ç½® DuQuant é…ç½® (OPENPI_DUQUANT_*)
3. å¯åŠ¨ Python è„šæœ¬

```bash
time python examples/libero/main.py \
  --args.headless \
  --args.policy-config pi05_libero \
  --args.policy-dir "$CKPT" \
  --args.task-suite-name "$TASK_SUITE" \
  --args.num-trials-per-task 20 \
  --args.seed "$SEED"
```

---

### é˜¶æ®µ 2: ä¸»ç¨‹åºå…¥å£ (main.py)

**æ–‡ä»¶**: `examples/libero/main.py`

**å‡½æ•°**: `eval_libero(args: Args)`

**è¡Œå·**: 171-438

#### 2.1 åˆå§‹åŒ–é˜¶æ®µ (è¡Œ 173-196)

```python
# è®¾ç½®éšæœºç§å­
np.random.seed(args.seed)

# åŠ è½½ LIBERO benchmark
benchmark_dict = benchmark.get_benchmark_dict()
task_suite = benchmark_dict[args.task_suite_name]()  # ä¾‹å¦‚: libero_spatial

# æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®æœ€å¤§æ­¥æ•°
if args.task_suite_name == "libero_spatial":
    max_steps = 220
elif args.task_suite_name == "libero_10":
    max_steps = 520
# ...
```

#### 2.2 åˆ›å»º Policy å¯¹è±¡ (è¡Œ 198-235)

**å…³é”®ä»£ç **:
```python
# å¯¼å…¥æœ¬åœ° policy æ¨¡å—
from openpi_client import local_policy as _local_policy
from openpi.policies import policy_config as _policy_config
from openpi.training import config as _config

# åŠ è½½ policy
policy_obj = _policy_config.create_trained_policy(
    _config.get_config(args.policy_config),  # åŠ è½½ pi05_libero é…ç½®
    args.policy_dir,                          # checkpoint ç›®å½•
    default_prompt=None,
)

# åŒ…è£…æˆ LocalPolicy å®¢æˆ·ç«¯
client = _local_policy.LocalPolicy(policy_obj)
```

**è°ƒç”¨é“¾**:
```
_policy_config.create_trained_policy()
    â†’ src/openpi/policies/policy_config.py:16
```

---

### é˜¶æ®µ 3: åˆ›å»º Policy (policy_config.py)

**æ–‡ä»¶**: `src/openpi/policies/policy_config.py`

**å‡½æ•°**: `create_trained_policy()`

**è¡Œå·**: 16-102

#### 3.1 æ£€æµ‹æ¨¡å‹ç±»å‹ (è¡Œ 48-50)

```python
# æ£€æŸ¥æ˜¯å¦æ˜¯ PyTorch æ¨¡å‹
weight_path = os.path.join(checkpoint_dir, "model.safetensors")
is_pytorch = os.path.exists(weight_path)
```

#### 3.2 åŠ è½½ PyTorch æ¨¡å‹ (è¡Œ 53-65)

```python
if is_pytorch:
    # è°ƒç”¨ ModelConfig.load_pytorch()
    model = train_config.model.load_pytorch(train_config, weight_path)

    # è½¬æ¢éƒ¨åˆ†å‚æ•°åˆ° bfloat16
    model.paligemma_with_expert.to_bfloat16_for_selected_params("bfloat16")

    # ğŸ”¥ å…³é”®æ­¥éª¤ï¼šå¯ç”¨ DuQuant é‡åŒ– ğŸ”¥
    from openpi.models_pytorch.duquant_layers import enable_duquant_if_configured
    enable_duquant_if_configured(model)
```

**è°ƒç”¨é“¾**:
```
train_config.model.load_pytorch()
    â†’ src/openpi/models/model.py:285

enable_duquant_if_configured()
    â†’ src/openpi/models_pytorch/duquant_layers.py:422
```

#### 3.3 åˆ›å»º Policy å¯¹è±¡ (è¡Œ 85-102)

```python
return _policy.Policy(
    model,
    transforms=[
        # è¾“å…¥ transforms: å›¾åƒé¢„å¤„ç†ã€æ ‡å‡†åŒ–ç­‰
        transforms.InjectDefaultPrompt(default_prompt),
        *data_config.data_transforms.inputs,
        transforms.Normalize(norm_stats, use_quantiles=data_config.use_quantile_norm),
        *data_config.model_transforms.inputs,
    ],
    output_transforms=[
        # è¾“å‡º transforms: åæ ‡å‡†åŒ–ç­‰
        *data_config.model_transforms.outputs,
        transforms.Unnormalize(norm_stats, use_quantiles=data_config.use_quantile_norm),
        *data_config.data_transforms.outputs,
    ],
    sample_kwargs=sample_kwargs,
    is_pytorch=is_pytorch,
    pytorch_device=pytorch_device,
)
```

---

### é˜¶æ®µ 4: åŠ è½½ PyTorch æ¨¡å‹ (model.py)

**æ–‡ä»¶**: `src/openpi/models/model.py`

**å‡½æ•°**: `load_pytorch()`

**è¡Œå·**: 285-289

```python
def load_pytorch(self, train_config, weight_path: str):
    logger.info(f"train_config: {train_config}")

    # åˆ›å»º PI0Pytorch æ¨¡å‹å®ä¾‹
    model = pi0_pytorch.PI0Pytorch(config=train_config.model)

    # ä» safetensors æ–‡ä»¶åŠ è½½æƒé‡
    safetensors.torch.load_model(model, weight_path)

    return model
```

**PI0Pytorch æ¨¡å‹ç»“æ„** (åœ¨ `src/openpi/models_pytorch/pi0_pytorch.py` ä¸­å®šä¹‰):

```python
class PI0Pytorch(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.paligemma_with_expert = PaliGemmaWithExpert(config)
        # paligemma_with_expert åŒ…å«:
        # - paligemma.vision_tower (SigLIP è§†è§‰ç¼–ç å™¨)
        # - paligemma.language_model (Gemma LLM)
        # - gemma_expert (DiT transformer)
```

---

### é˜¶æ®µ 5: å¯ç”¨ DuQuant é‡åŒ– (duquant_layers.py)

**æ–‡ä»¶**: `src/openpi/models_pytorch/duquant_layers.py`

**å‡½æ•°**: `enable_duquant_if_configured(model)`

**è¡Œå·**: 422-467

#### 5.1 æ£€æŸ¥ç¯å¢ƒå˜é‡ (è¡Œ 430-434)

```python
env = os.environ
keys = [k for k in env.keys() if k.startswith("OPENPI_DUQUANT_")]
activate = any(k not in ("OPENPI_DUQUANT_PACKDIR",) for k in keys)
if not activate:
    return  # æ²¡æœ‰è®¾ç½® DuQuant é…ç½®ï¼Œç›´æ¥è¿”å›
```

#### 5.2 è¯»å–é…ç½® (è¡Œ 437-444)

```python
scope = env.get("OPENPI_DUQUANT_SCOPE", "policy.dit.")
inc = env.get("OPENPI_DUQUANT_INCLUDE", r".*(q_proj|k_proj|v_proj|o_proj|...).*")
exc = env.get("OPENPI_DUQUANT_EXCLUDE", r"(?:^|\.)(norm|ln|...)(?:\.|$)")
per_layer_wbits = _parse_per_layer_wbits(env.get("OPENPI_DUQUANT_WBITS"))
dry_run = env.get("OPENPI_DUQUANT_DRYRUN", "0") not in ("0", "false", "False")

# åˆ›å»º DuQuant é…ç½®å¯¹è±¡
cfg = DuQuantConfig()
```

**DuQuantConfig é»˜è®¤å€¼** (è¡Œ 28-62):
```python
@dataclasses.dataclass
class DuQuantConfig:
    weight_bits: int = int(os.environ.get("OPENPI_DUQUANT_WBITS_DEFAULT", "4"))
    act_bits: int = int(os.environ.get("OPENPI_DUQUANT_ABITS", "0"))  # 0=ç¦ç”¨æ¿€æ´»é‡åŒ–
    block_size: int = int(os.environ.get("OPENPI_DUQUANT_BLOCK", "128"))
    block_out_size: int | None = None  # è¾“å‡ºé€šé“çš„ block size (é»˜è®¤åŒ block_size)
    enable_permute: bool = os.environ.get("OPENPI_DUQUANT_PERMUTE", "0") not in ("0", "false", "False")
    row_rot_mode: str = os.environ.get("OPENPI_DUQUANT_ROW_ROT", "disabled")  # disabled/restore/propagate
    act_percentile: float = float(os.environ.get("OPENPI_DUQUANT_ACT_PCT", "99.9"))
    calib_steps: int = int(os.environ.get("OPENPI_DUQUANT_CALIB_STEPS", "32"))
    lambda_smooth: float = float(os.environ.get("OPENPI_DUQUANT_LS", "0.5"))
```

#### 5.3 é€‰æ‹©ç›®æ ‡å±‚ (è¡Œ 447-454)

```python
# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å±‚å
targets = select_targets(
    model,
    include_regex=inc,   # åŒ¹é…éœ€è¦é‡åŒ–çš„å±‚
    exclude_regex=exc,   # æ’é™¤ä¸éœ€è¦é‡åŒ–çš„å±‚
    scope_prefix=scope,  # åªåœ¨æŒ‡å®š scope å†…æœç´¢
    whitelist=whitelist_list,
)
```

**select_targets() å‡½æ•°** (è¡Œ 351-377):
```python
def select_targets(model, *, include_regex, exclude_regex, scope_prefix, ...):
    inc = re.compile(include_regex)
    exc = re.compile(exclude_regex)
    results = []

    # éå†æ¨¡å‹æ‰€æœ‰æ¨¡å—
    for name, mod in model.named_modules():
        if not isinstance(mod, nn.Linear):
            continue  # åªå¤„ç† Linear å±‚

        # æ£€æŸ¥æ˜¯å¦åœ¨ scope å†…
        if scope_prefix is not None and not name.startswith(scope_prefix):
            continue

        # æ£€æŸ¥æ˜¯å¦åŒ¹é… INCLUDE æ­£åˆ™
        if not inc.search(name):
            continue

        # æ£€æŸ¥æ˜¯å¦åŒ¹é… EXCLUDE æ­£åˆ™
        if exc.search(name):
            continue

        results.append((name, mod))

    return results
```

**ä½ çš„é…ç½®ä¼šåŒ¹é…**:
- âœ… `language_model.layers.*.self_attn.q_proj` (LLM attention)
- âœ… `language_model.layers.*.mlp.gate_proj` (LLM MLP)
- âœ… `gemma_expert.model.layers.*.mlp.gate_proj` (DiT MLP)
- âŒ `gemma_expert.model.layers.*.self_attn.q_proj` (DiT attention - è¢« EXCLUDE æ’é™¤)

#### 5.4 æ›¿æ¢ä¸º DuQuantLinear (è¡Œ 456-467)

```python
if targets:
    print(f"[DUQUANT] Matched Linear layers: {len(targets)}")

    # è°ƒç”¨ wrap_duquant æ›¿æ¢å±‚
    wrap_duquant(
        model,
        layer_names=[name for name, _ in targets],
        cfg=cfg,
        per_layer_wbits=per_layer_wbits,
        dry_run=dry_run,
    )
```

**wrap_duquant() å‡½æ•°** (è¡Œ 380-419):
```python
def wrap_duquant(model, layer_names, cfg, per_layer_wbits, dry_run):
    replaced = 0
    for name in layer_names:
        # è·å–çˆ¶æ¨¡å—å’Œå±æ€§å
        parent, attr = _get_parent_module_and_attr(model, name)
        mod = getattr(parent, attr)  # åŸå§‹çš„ nn.Linear

        if dry_run:
            print(f"[DUQUANT][DRYRUN] {name}: Linear({mod.in_features}->{mod.out_features}) ...")
            continue

        # åˆ›å»º DuQuantLinear åŒ…è£…å™¨
        dq = DuQuantLinear(mod, name=name, cfg=cfg, weight_bits=wbits)

        # æ›¿æ¢åŸå§‹å±‚
        setattr(parent, attr, dq)

        print(f"[DUQUANT][REPLACED] {name}: Linear(...) -> DuQuantLinear W{wbits} A{cfg.act_bits}")
        replaced += 1

    print(f"[DUQUANT] Total layers replaced: {replaced}")
```

---

### é˜¶æ®µ 6: DuQuantLinear åˆå§‹åŒ–

**æ–‡ä»¶**: `src/openpi/models_pytorch/duquant_layers.py`

**ç±»**: `DuQuantLinear`

**è¡Œå·**: 66-188

#### 6.1 æ„é€ å‡½æ•° (è¡Œ 66-121)

```python
class DuQuantLinear(nn.Module):
    def __init__(
        self,
        orig_linear: nn.Linear,
        *,
        name: str,
        cfg: DuQuantConfig,
        weight_bits: int,
    ):
        super().__init__()
        self.name = name
        self.cfg = cfg
        self.weight_bits = weight_bits

        # ä¿å­˜åŸå§‹ Linear å±‚çš„å‚æ•°
        self.in_features = orig_linear.in_features
        self.out_features = orig_linear.out_features
        self.bias = orig_linear.bias

        # ğŸ”¥ å…³é”®ï¼šå°è¯•ä» pack æ–‡ä»¶åŠ è½½é¢„è®¡ç®—çš„å˜æ¢çŸ©é˜µ
        packdir = os.environ.get("OPENPI_DUQUANT_PACKDIR")
        pack_path = Path(packdir) / f"{name}.npz" if packdir else None

        if pack_path and pack_path.exists():
            # ä»ç£ç›˜åŠ è½½
            self.pack = PackResult(**dict(np.load(pack_path, allow_pickle=True)))
            print(f"[DUQUANT][LOADED] {name}: pack from {pack_path}")
        else:
            # ç¬¬ä¸€æ¬¡è¿è¡Œï¼šåœ¨çº¿è®¡ç®—å¹¶ä¿å­˜
            self.pack = duquant_pack_single_layer(
                orig_linear.weight.detach(),
                name=name,
                cfg=cfg,
                weight_bits=weight_bits,
            )
            if pack_path:
                np.savez(pack_path, **dataclasses.asdict(self.pack))
                print(f"[DUQUANT][PACKED] {name}: saved to {pack_path}")
```

#### 6.2 Pack æ–‡ä»¶å†…å®¹ (duquant_preprocess.py)

**PackResult æ•°æ®ç»“æ„** (è¡Œ 140-149):
```python
@dataclass
class PackResult:
    # è¾“å…¥ä¾§å˜æ¢ (åˆ—å˜æ¢)
    R_in_blocks: Dict[int, np.ndarray]   # block_index -> æ—‹è½¬çŸ©é˜µ R_in (BxB)
    perm: np.ndarray                      # æ’åˆ—ç´¢å¼• (é€šé“é‡æ’)

    # è¾“å‡ºä¾§å˜æ¢ (è¡Œå˜æ¢)
    R_out_blocks: Dict[int, np.ndarray]  # block_index -> æ—‹è½¬çŸ©é˜µ R_out (BxB)

    # é‡åŒ–å‚æ•°
    weight_scale: np.ndarray              # æ¯ä¸ªè¾“å‡ºé€šé“çš„é‡åŒ– scale
    meta: Dict[str, Any]                  # å…ƒæ•°æ® (block_size, lambda_smooth ç­‰)
```

**duquant_pack_single_layer() å‡½æ•°** (è¡Œ 673-836):
```python
def duquant_pack_single_layer(weight_tensor, *, name, cfg, weight_bits):
    """
    å¯¹å•ä¸ª Linear å±‚æ‰§è¡Œ DuQuant é¢„å¤„ç†ï¼Œè®¡ç®—æ—‹è½¬çŸ©é˜µã€æ’åˆ—å’Œé‡åŒ– scale

    æ­¥éª¤:
    1. è¾“å…¥é€šé“åˆ†å— (block_size)
    2. æ¯ä¸ª block è®¡ç®—æ—‹è½¬çŸ©é˜µ R_in (ä½¿ç”¨ SVD)
    3. è®¡ç®— zigzag æ’åˆ— (åŸºäºæƒé‡èƒ½é‡)
    4. è®¡ç®—è¾“å‡ºæ—‹è½¬çŸ©é˜µ R_out (å¯é€‰)
    5. è®¡ç®—é‡åŒ– scale (æ¯ä¸ªè¾“å‡ºé€šé“)
    """
    W = weight_tensor.cpu().numpy().astype(np.float32)
    out_features, in_features = W.shape
    block_size = cfg.block_size

    # æ­¥éª¤ 1: è¾“å…¥é€šé“åˆ†å—ï¼Œè®¡ç®—æ—‹è½¬çŸ©é˜µ
    R_in_blocks = {}
    n_blocks = (in_features + block_size - 1) // block_size
    for b in range(n_blocks):
        start = b * block_size
        end = min((b + 1) * block_size, in_features)
        W_block = W[:, start:end]

        # è®¡ç®—è¯¥ block çš„æ—‹è½¬çŸ©é˜µ (ä½¿ç”¨ SVD: W_block = U @ S @ Vt)
        R = compute_block_rotation(W_block)  # è¿”å› Vt çš„å‰å‡ è¡Œ
        R_in_blocks[b] = R

    # æ­¥éª¤ 2: åº”ç”¨æ—‹è½¬
    W_rotated = apply_rotation_to_weight(W, R_in_blocks, block_size)

    # æ­¥éª¤ 3: è®¡ç®— zigzag æ’åˆ—
    if cfg.enable_permute:
        perm = compute_zigzag_permutation(W_rotated, block_size, cfg.lambda_smooth)
        W_permuted = W_rotated[:, perm]
    else:
        perm = None
        W_permuted = W_rotated

    # æ­¥éª¤ 4: è¾“å‡ºæ—‹è½¬ (å¯é€‰)
    R_out_blocks = None
    if cfg.row_rot_mode != "disabled":
        R_out_blocks = compute_output_rotation(W_permuted, cfg.block_out_size)

    # æ­¥éª¤ 5: è®¡ç®—é‡åŒ– scale
    weight_scale = compute_weight_scale(W_permuted, weight_bits)

    return PackResult(
        R_in_blocks=R_in_blocks,
        perm=perm,
        R_out_blocks=R_out_blocks,
        weight_scale=weight_scale,
        meta={"block_size": block_size, "lambda_smooth": cfg.lambda_smooth},
    )
```

#### 6.3 é¢„è®¡ç®—å—å¯¹è§’çŸ©é˜µ (è¡Œ 142-188)

```python
# æ‰¹é‡æ—‹è½¬ä¼˜åŒ–ï¼šå°† 128 ä¸ªå°çŸ©é˜µä¹˜æ³•åˆå¹¶æˆ 1 ä¸ªå¤§çŸ©é˜µä¹˜æ³•
self._use_batched_rotation = os.environ.get("OPENPI_DUQUANT_BATCH_ROT", "1") not in ("0", "false", "False")
self.register_buffer("_R_in_all", None)
self.register_buffer("_R_out_all", None)

if self._use_batched_rotation:
    self._precompute_block_diagonal_matrices()
```

**_precompute_block_diagonal_matrices() å‡½æ•°** (è¡Œ 142-188):
```python
def _precompute_block_diagonal_matrices(self):
    """
    é¢„è®¡ç®—å—å¯¹è§’çŸ©é˜µï¼ŒåŠ é€Ÿå‰å‘ä¼ æ’­

    åŸå§‹æ–¹æ³•: å¯¹æ¯ä¸ª block åˆ†åˆ«åšçŸ©é˜µä¹˜æ³•
        for b in range(n_blocks):
            x_block = x[:, b*B:(b+1)*B]
            x_rot = x_block @ R_in[b]

    ä¼˜åŒ–æ–¹æ³•: æ„é€ ä¸€ä¸ªå¤§çš„å—å¯¹è§’çŸ©é˜µï¼Œä¸€æ¬¡çŸ©é˜µä¹˜æ³•å®Œæˆ
        R_all = block_diag(R_in[0], R_in[1], ..., R_in[n-1])
        x_rot = x @ R_all

    æ€§èƒ½æå‡: 256 æ¬¡å° matmul -> 2 æ¬¡å¤§ matmul (10-20x åŠ é€Ÿ)
    """
    if self._R_in_block_indices:
        # æ„å»ºè¾“å…¥æ—‹è½¬çš„å—å¯¹è§’çŸ©é˜µ
        R_list = []
        for b in range(n_blocks):
            R = getattr(self, f"_R_in_{b}")
            R_list.append(R)

        # ä½¿ç”¨ torch.block_diag æ„é€ å—å¯¹è§’çŸ©é˜µ
        R_all = torch.block_diag(*R_list)
        self._R_in_all = R_all

    # åŒæ ·å¤„ç†è¾“å‡ºæ—‹è½¬
    if self._R_out_block_indices:
        R_out_list = [getattr(self, f"_R_out_{b}") for b in range(n_out_blocks)]
        R_out_all = torch.block_diag(*R_out_list)
        self._R_out_all = R_out_all
```

---

### é˜¶æ®µ 7: LIBERO ç¯å¢ƒå¾ªç¯ (main.py)

**æ–‡ä»¶**: `examples/libero/main.py`

**è¡Œå·**: 245-398

#### 7.1 å¤–å±‚å¾ªç¯ï¼šéå†æ‰€æœ‰ä»»åŠ¡ (è¡Œ 245-397)

```python
for task_id in tqdm.tqdm(range(num_tasks_in_suite)):
    # è·å–ä»»åŠ¡æè¿°
    task = task_suite.get_task(task_id)
    task_description = task.language  # ä¾‹å¦‚: "put the red mug on the plate"

    # è·å–åˆå§‹çŠ¶æ€
    initial_states = task_suite.get_task_init_states(task_id)

    # åˆ›å»ºç¯å¢ƒ
    env, task_description = _get_libero_env(task, LIBERO_ENV_RESOLUTION, args.seed)
```

#### 7.2 å†…å±‚å¾ªç¯ï¼šæ¯ä¸ªä»»åŠ¡çš„å¤šæ¬¡è¯•éªŒ (è¡Œ 257-393)

```python
    for episode_idx in range(args.num_trials_per_task):  # é»˜è®¤ 20 æ¬¡
        # é‡ç½®ç¯å¢ƒ
        env.reset()
        action_plan = collections.deque()  # å­˜å‚¨åŠ¨ä½œåºåˆ—

        # è®¾ç½®åˆå§‹çŠ¶æ€
        obs = env.set_init_state(initial_states[episode_idx])

        t = 0
        replay_images = []
        episode_infer_ms = []
```

#### 7.3 æ—¶é—´æ­¥å¾ªç¯ (è¡Œ 273-354)

```python
        while t < max_steps + args.num_steps_wait:
            # å‰ 10 æ­¥ç­‰å¾…ç‰©ä½“ç¨³å®š (ç‰©ç†ä»¿çœŸéœ€è¦æ—¶é—´)
            if t < args.num_steps_wait:
                obs, reward, done, info = env.step(LIBERO_DUMMY_ACTION)
                t += 1
                continue

            # è·å–å›¾åƒè§‚æµ‹
            img = obs["agentview_image"][::-1, ::-1]  # æ—‹è½¬ 180 åº¦
            wrist_img = obs["robot0_eye_in_hand_image"][::-1, ::-1]

            # é¢„å¤„ç†å›¾åƒ
            img = image_tools.resize_with_pad(img, 224, 224)
            wrist_img = image_tools.resize_with_pad(wrist_img, 224, 224)
```

#### 7.4 æ¨¡å‹æ¨ç† (è¡Œ 296-340)

```python
            if not action_plan:
                # åŠ¨ä½œé˜Ÿåˆ—ä¸ºç©ºï¼Œéœ€è¦é‡æ–°è§„åˆ’

                # å‡†å¤‡è¾“å…¥æ•°æ®
                element = {
                    "observation/image": img,
                    "observation/wrist_image": wrist_img,
                    "observation/state": np.concatenate((
                        obs["robot0_eef_pos"],        # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½® (3D)
                        _quat2axisangle(obs["robot0_eef_quat"]),  # æ—‹è½¬ (è½´è§’, 3D)
                        obs["robot0_gripper_qpos"],   # å¤¹çˆªå¼€åˆ (1D)
                    )),
                    "prompt": task_description,  # ä»»åŠ¡æè¿°æ–‡æœ¬
                }

                # ğŸ”¥ è°ƒç”¨æ¨¡å‹æ¨ç† ğŸ”¥
                call_start = time.perf_counter()
                infer_result = client.infer(element)
                elapsed_ms = (time.perf_counter() - call_start) * 1000.0

                # è·å–åŠ¨ä½œåºåˆ— (ä¾‹å¦‚: shape [15, 7])
                action_chunk = infer_result["actions"]

                # åªä½¿ç”¨å‰ 5 æ­¥ (replan_steps)
                action_plan.extend(action_chunk[:args.replan_steps])

            # ä»é˜Ÿåˆ—ä¸­å–å‡ºä¸€ä¸ªåŠ¨ä½œ
            action = action_plan.popleft()

            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = env.step(action.tolist())

            if done:  # ä»»åŠ¡æˆåŠŸ
                task_successes += 1
                break
            t += 1
```

---

### é˜¶æ®µ 8: Policy æ¨ç† (local_policy.py + policy.py)

**æ–‡ä»¶**: `src/openpi_client/local_policy.py`

**å‡½æ•°**: `LocalPolicy.infer()`

```python
class LocalPolicy:
    def __init__(self, policy_obj):
        self._policy = policy_obj

    def infer(self, element):
        """
        æ‰§è¡Œæ¨ç†

        Args:
            element: åŒ…å« observation å’Œ prompt çš„å­—å…¸

        Returns:
            {
                "actions": numpy array of shape [action_horizon, action_dim],
                "policy_timing": {"infer_ms": ...}
            }
        """
        # è°ƒç”¨ Policy å¯¹è±¡çš„ sample_actions
        actions = self._policy.sample_actions(element)
        return {"actions": actions}
```

**æ–‡ä»¶**: `src/openpi/policies/policy.py`

**å‡½æ•°**: `Policy.sample_actions()`

```python
class Policy:
    def __init__(self, model, transforms, output_transforms, sample_kwargs, ...):
        self.model = model
        self.transforms = transforms
        self.output_transforms = output_transforms
        self._is_pytorch_model = is_pytorch

    def sample_actions(self, data):
        """
        å®Œæ•´çš„æ¨ç†æµç¨‹

        æ­¥éª¤:
        1. åº”ç”¨è¾“å…¥ transforms
        2. è°ƒç”¨æ¨¡å‹å‰å‘ä¼ æ’­
        3. åº”ç”¨è¾“å‡º transforms
        """
        # æ­¥éª¤ 1: è¾“å…¥é¢„å¤„ç†
        for transform in self.transforms:
            data = transform(data)

        # æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        batch = self._prepare_batch(data)

        # æ­¥éª¤ 2: æ¨¡å‹å‰å‘ä¼ æ’­
        if self._is_pytorch_model:
            with torch.no_grad():
                output = self.model(batch)  # ğŸ”¥ è°ƒç”¨ PI0Pytorch.forward()
        else:
            output = self.model(batch)

        # æ­¥éª¤ 3: è¾“å‡ºåå¤„ç†
        actions = output["actions"]
        for transform in self.output_transforms:
            actions = transform(actions)

        return actions
```

---

### é˜¶æ®µ 9: æ¨¡å‹å‰å‘ä¼ æ’­ (pi0_pytorch.py)

**æ–‡ä»¶**: `src/openpi/models_pytorch/pi0_pytorch.py`

**ç±»**: `PI0Pytorch`

**å‡½æ•°**: `forward()`

```python
class PI0Pytorch(nn.Module):
    def __init__(self, config):
        super().__init__()
        # ä¸»æ¨¡å‹ï¼šPaliGemma + Expert DiT
        self.paligemma_with_expert = PaliGemmaWithExpert(config)

    def forward(self, batch):
        """
        å‰å‘ä¼ æ’­

        è¾“å…¥:
            batch = {
                "observation": {
                    "image": [B, 224, 224, 3],
                    "wrist_image": [B, 224, 224, 3],
                    "state": [B, 7],  # eef_pos(3) + eef_rot(3) + gripper(1)
                },
                "prompt": [B, max_token_len],  # tokenized text
            }

        è¾“å‡º:
            {
                "actions": [B, action_horizon, action_dim],
            }
        """
        # è°ƒç”¨ PaliGemmaWithExpert
        return self.paligemma_with_expert(batch)
```

**PaliGemmaWithExpert ç»“æ„**:
```python
class PaliGemmaWithExpert(nn.Module):
    def __init__(self, config):
        super().__init__()
        # è§†è§‰ç¼–ç å™¨ (SigLIP)
        self.paligemma = PaliGemmaForConditionalGeneration(...)

        # DiT expert (åŠ¨ä½œé¢„æµ‹)
        self.gemma_expert = GemmaExpert(config)

    def forward(self, batch):
        # 1. ç¼–ç å›¾åƒ
        vision_features = self.paligemma.vision_tower(batch["observation"]["image"])

        # 2. LLM å¤„ç†æ–‡æœ¬ + è§†è§‰ç‰¹å¾
        text_embeddings = self.paligemma.language_model(
            input_ids=batch["prompt"],
            vision_features=vision_features,
        )

        # 3. DiT é¢„æµ‹åŠ¨ä½œ
        # ğŸ”¥ è¿™é‡Œä¼šç»è¿‡ DuQuantLinear å±‚ ğŸ”¥
        actions = self.gemma_expert(
            text_embeddings=text_embeddings,
            state=batch["observation"]["state"],
        )

        return {"actions": actions}
```

---

### é˜¶æ®µ 10: DuQuantLinear å‰å‘ä¼ æ’­

**æ–‡ä»¶**: `src/openpi/models_pytorch/duquant_layers.py`

**å‡½æ•°**: `DuQuantLinear.forward()`

**è¡Œå·**: 280-340

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    """
    DuQuant å‰å‘ä¼ æ’­

    æ­¥éª¤:
    1. åº”ç”¨è¾“å…¥å˜æ¢ (æ—‹è½¬ + æ’åˆ—)
    2. æ¿€æ´»é‡åŒ– (å¦‚æœå¯ç”¨)
    3. æƒé‡é‡åŒ–
    4. çŸ©é˜µä¹˜æ³•
    5. åº”ç”¨è¾“å‡ºå˜æ¢ (æ—‹è½¬æ¢å¤)
    """
    # æ­¥éª¤ 1: è¾“å…¥å˜æ¢ (ä½¿ç”¨é¢„è®¡ç®—çš„å—å¯¹è§’çŸ©é˜µ)
    if self._use_batched_rotation and self._R_in_all is not None:
        original_shape = x.shape
        x_t = x.reshape(-1, self.in_features)

        # åº”ç”¨æ’åˆ—
        if self._perm_cache is not None:
            x_t = x_t.index_select(dim=-1, index=self._perm_cache)

        # åº”ç”¨æ‰¹é‡è¾“å…¥æ—‹è½¬ (å•æ¬¡å¤§ matmul)
        x_t = x_t @ self._R_in_all  # ğŸš€ 10-20x åŠ é€Ÿ
        x_t = x_t.reshape(*original_shape)
    else:
        # å›é€€ï¼šé€å—æ—‹è½¬ (æ…¢)
        from .duquant_preprocess import apply_input_transform_optimized
        x_t = apply_input_transform_optimized(x, self.pack, ...)

    # æ­¥éª¤ 2: æ¿€æ´»é‡åŒ– (fake quantization)
    if self.cfg.act_bits > 0:
        s_a = self._get_act_scale(x_t)  # è·å– activation scale
        x_t = fake_quantize_sym(x_t, s_a, self.cfg.act_bits, label="activation_forward")

    # æ­¥éª¤ 3: æƒé‡é‡åŒ– + çŸ©é˜µä¹˜æ³•
    if self._weight_quantized_cached:
        # ä½¿ç”¨é¢„é‡åŒ–çš„æƒé‡ (ç¬¬äºŒæ¬¡å‰å‘ä¼ æ’­å¼€å§‹ä½¿ç”¨)
        y_lin = torch.nn.functional.linear(x_t, self._W_t_quantized, None)
    elif self.weight_bits > 0:
        # ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼šåœ¨çº¿é‡åŒ–
        y_lin = torch.nn.functional.linear(
            x_t,
            fake_quantize_sym(
                self._W_t,
                self._w_scales[:, None],
                self.weight_bits,
                label="weight_fallback",
            ),
            None
        )
    else:
        y_lin = torch.nn.functional.linear(x_t, self._W_t, None)

    # æ­¥éª¤ 4: è¾“å‡ºæ—‹è½¬æ¢å¤
    if self.cfg.row_rot_mode == "restore" and self.pack.R_out_blocks is not None:
        from .duquant_preprocess import apply_output_restore_optimized
        y_lin = apply_output_restore_optimized(
            y_lin, self.pack, self._get_R_out_cache(), self._block_out_size
        )
        # æ¢å¤åå†åŠ  bias
        if self.bias is not None:
            y_lin = y_lin + self.bias
    else:
        # ä¼ æ’­æ¨¡å¼æˆ–ç¦ç”¨ï¼šbias åœ¨å½“å‰åŸºä¸‹
        if self.bias is not None:
            y_lin = y_lin + self.bias

    return y_lin
```

**fake_quantize_sym() å‡½æ•°** (duquant_preprocess.py:120-137):
```python
def fake_quantize_sym(x, scale, bits, *, label=None):
    """
    å¯¹ç§°é‡åŒ– (symmetric quantization)

    å…¬å¼:
        x_quant = clamp(round(x / scale), -max_q-1, max_q) * scale

    å…¶ä¸­ max_q = 2^(bits-1) - 1
    ä¾‹å¦‚: 4-bit -> max_q = 7, èŒƒå›´ [-8, 7]
          8-bit -> max_q = 127, èŒƒå›´ [-128, 127]
    """
    if bits <= 0:
        return x

    max_q = 2**(bits-1) - 1  # qmax(bits)
    x_scaled = x / scale      # å½’ä¸€åŒ–
    x_clamped = torch.clamp(torch.round(x_scaled), -max_q - 1, max_q)  # é‡åŒ– + æˆªæ–­
    return x_clamped * scale  # åé‡åŒ– (fake quantization)
```

---

## 3. å…³é”®æ•°æ®æµ

### 3.1 å›¾åƒæ•°æ®æµ

```
åŸå§‹å›¾åƒ (256x256x3)
    â†“
æ—‹è½¬ 180Â° (LIBERO é¢„å¤„ç†)
    â†“
Resize with padding (224x224x3)
    â†“
Normalize (transforms.Normalize)
    â†“
SigLIP Vision Encoder
    â†“
Vision features [B, num_patches, hidden_dim]
```

### 3.2 æ–‡æœ¬æ•°æ®æµ

```
Task description (string)
    â†“
Tokenize (Gemma tokenizer)
    â†“
Token IDs [B, max_token_len]
    â†“
Gemma LLM (language_model)
    â†“
Text embeddings [B, seq_len, hidden_dim]
```

### 3.3 çŠ¶æ€æ•°æ®æµ

```
Robot state (7D)
    â”œâ”€ end_effector_pos (3D)
    â”œâ”€ end_effector_rot_axisangle (3D)
    â””â”€ gripper_position (1D)
    â†“
Normalize (transforms.Normalize)
    â†“
Concatenate with embeddings
    â†“
DiT Transformer
```

### 3.4 åŠ¨ä½œè¾“å‡ºæµ

```
DiT output [B, action_horizon, action_dim]
    â†“
Unnormalize (transforms.Unnormalize)
    â†“
åŠ¨ä½œåºåˆ— [15, 7]
    â”œâ”€ end_effector_delta_pos (3D)
    â”œâ”€ end_effector_delta_rot (3D)
    â””â”€ gripper_command (1D)
```

---

## 4. DuQuant é‡åŒ–æµç¨‹è¯¦è§£

### 4.1 ç¦»çº¿ Packing é˜¶æ®µ (ç¬¬ä¸€æ¬¡è¿è¡Œ)

```
åŸå§‹æƒé‡ W [out_features, in_features]
    â†“
ã€æ­¥éª¤ 1ã€‘è¾“å…¥é€šé“åˆ†å— + è®¡ç®—æ—‹è½¬çŸ©é˜µ
    for each block:
        W_block = W[:, b*block_size:(b+1)*block_size]
        U, S, Vt = SVD(W_block.T @ W_block)
        R_in[b] = Vt[:block_size, :]
    â†“
ã€æ­¥éª¤ 2ã€‘åº”ç”¨æ—‹è½¬
    W_rotated = W @ block_diag(R_in[0], R_in[1], ...)
    â†“
ã€æ­¥éª¤ 3ã€‘è®¡ç®— zigzag æ’åˆ—
    energy = sum(W_rotated^2, axis=0)  # æ¯ä¸ªè¾“å…¥é€šé“çš„èƒ½é‡
    perm = zigzag_permute(energy, block_size, lambda_smooth)
    W_permuted = W_rotated[:, perm]
    â†“
ã€æ­¥éª¤ 4ã€‘è¾“å‡ºæ—‹è½¬ (å¯é€‰)
    for each output block:
        compute R_out[b] using similar SVD
    â†“
ã€æ­¥éª¤ 5ã€‘è®¡ç®—é‡åŒ– scale
    weight_scale = max(abs(W_permuted), axis=1) / qmax(weight_bits)
    â†“
ä¿å­˜åˆ° pack æ–‡ä»¶ (.npz)
    - R_in_blocks
    - perm
    - R_out_blocks
    - weight_scale
    - meta
```

### 4.2 åœ¨çº¿æ¨ç†é˜¶æ®µ (åç»­è¿è¡Œ)

```
è¾“å…¥æ¿€æ´» x [batch, in_features]
    â†“
ã€æ­¥éª¤ 1ã€‘åº”ç”¨è¾“å…¥å˜æ¢
    x_perm = x[:, perm]  # æ’åˆ—
    x_rot = x_perm @ R_in_all  # æ—‹è½¬ (ä½¿ç”¨é¢„è®¡ç®—çš„å—å¯¹è§’çŸ©é˜µ)
    â†“
ã€æ­¥éª¤ 2ã€‘æ¿€æ´»é‡åŒ– (A8)
    scale_a = percentile(abs(x_rot), 99.9) / 127
    x_quant = fake_quantize_sym(x_rot, scale_a, 8)
    â†“
ã€æ­¥éª¤ 3ã€‘æƒé‡é‡åŒ– (W4) + çŸ©é˜µä¹˜æ³•
    W_quant = fake_quantize_sym(W_transformed, weight_scale, 4)
    y = x_quant @ W_quant.T
    â†“
ã€æ­¥éª¤ 4ã€‘è¾“å‡ºæ—‹è½¬æ¢å¤ (å¦‚æœå¯ç”¨)
    y_restored = y @ R_out_all.T
    â†“
è¾“å‡ºæ¿€æ´» [batch, out_features]
```

---

## 5. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 5.1 æ‰¹é‡æ—‹è½¬ä¼˜åŒ– (Batched Rotation)

**é—®é¢˜**: åŸå§‹å®ç°éœ€è¦å¯¹æ¯ä¸ª block å•ç‹¬åšçŸ©é˜µä¹˜æ³•
```python
# æ…¢: 128 æ¬¡å° matmul
for b in range(128):
    x_rot_b = x[:, b*16:(b+1)*16] @ R_in[b]  # [B, 16] @ [16, 16]
```

**è§£å†³æ–¹æ¡ˆ**: é¢„è®¡ç®—å—å¯¹è§’çŸ©é˜µ
```python
# å¿«: 1 æ¬¡å¤§ matmul
R_in_all = torch.block_diag(R_in[0], R_in[1], ..., R_in[127])  # [2048, 2048]
x_rot = x @ R_in_all  # [B, 2048] @ [2048, 2048]
```

**åŠ é€Ÿæ¯”**: 10-20x (å‡å°‘ GPU kernel å¯åŠ¨å¼€é”€)

### 5.2 æƒé‡é¢„é‡åŒ–ç¼“å­˜

**é—®é¢˜**: æ¯æ¬¡å‰å‘ä¼ æ’­éƒ½é‡åŒ–æƒé‡å¾ˆæ…¢
```python
# æ…¢: æ¯æ¬¡éƒ½é‡åŒ–
y = F.linear(x, fake_quantize_sym(W, scale, 4))
```

**è§£å†³æ–¹æ¡ˆ**: ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­åç¼“å­˜é‡åŒ–æƒé‡
```python
# å¿«: åªé‡åŒ–ä¸€æ¬¡
if not self._weight_quantized_cached:
    self._W_t_quantized = fake_quantize_sym(self._W_t, self._w_scales, 4)
    self._weight_quantized_cached = True

y = F.linear(x, self._W_t_quantized)
```

### 5.3 æ¿€æ´»é‡åŒ– calibration

**é—®é¢˜**: æ¿€æ´»çš„ scale éœ€è¦æ ¹æ®æ•°æ®åˆ†å¸ƒç¡®å®š

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ calibration é˜¶æ®µæ”¶é›†ç»Ÿè®¡ä¿¡æ¯
```python
# å‰ 32 æ­¥æ”¶é›†æ¿€æ´»ç»Ÿè®¡
if self.calibrator is not None and not self.calibrator.is_full():
    self.calibrator.observe(x)
    if self.calibrator.is_full():
        scale = compute_scale_from_calibration()
```

### 5.4 CUDA å†…å­˜ä¼˜åŒ–

**é—®é¢˜**: å—å¯¹è§’çŸ©é˜µå¢åŠ å†…å­˜å ç”¨ (~2GB for 126 layers Ã— 16MB)

**è§£å†³æ–¹æ¡ˆ**: å¯ç”¨ PyTorch expandable segments
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

---

## 6. è°ƒè¯•æŠ€å·§

### 6.1 æ‰“å°å±‚ä¿¡æ¯

```bash
# æ‰“å°æ‰€æœ‰è¢«é‡åŒ–çš„å±‚
export OPENPI_DUQUANT_DEBUG=1

# Dry-run æ¨¡å¼ï¼šåªæ‰“å°ä¸æ›¿æ¢
export OPENPI_DUQUANT_DRYRUN=1
```

### 6.2 æ€§èƒ½åˆ†æ

```bash
# å¯ç”¨ DuQuant profiling
export OPENPI_DUQUANT_PROFILE=1

# å¯ç”¨ Policy æ¨ç† profiling
export OPENPI_POLICY_PROFILE=1
```

è¾“å‡ºç¤ºä¾‹:
```
[DUQUANT][PROFILE] fake quantization summary
Label                    Calls    Total ms    Avg ms    Elems       GB/s
activation_forward       1234     123.45      0.100     12345678    10.23
weight_quantize          126      45.67       0.362     9876543     8.91
```

### 6.3 æ‰“å° Linear å±‚å½¢çŠ¶

```bash
export OPENPI_PRINT_LINEAR_SHAPES=1
```

è¾“å‡ºç¤ºä¾‹:
```
[LINEAR-MM] language_model.layers.0.self_attn.q_proj: x[1, 1024] @ W[2048, 1024]
[LINEAR-MM] gemma_expert.model.layers.0.mlp.gate_proj: x[1, 1024] @ W[4096, 1024]
```

---

## 7. å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆç¬¬ä¸€æ¬¡è¿è¡Œå¾ˆæ…¢ï¼Ÿ
**A**: ç¬¬ä¸€æ¬¡éœ€è¦è®¡ç®—æ—‹è½¬çŸ©é˜µå¹¶ä¿å­˜ pack æ–‡ä»¶ã€‚åç»­è¿è¡Œä¼šç›´æ¥åŠ è½½ pack æ–‡ä»¶ï¼Œé€Ÿåº¦å¿«å¾ˆå¤šã€‚

### Q2: Pack æ–‡ä»¶ä¿å­˜åœ¨å“ªé‡Œï¼Ÿ
**A**: `$OPENPI_DUQUANT_PACKDIR/<layer_name>.npz`

ä¾‹å¦‚: `duquant_packed_llm_dit_mlp_w4a8/paligemma_with_expert.paligemma.model.language_model.layers.0.mlp.gate_proj.npz`

### Q3: å¦‚ä½•éªŒè¯é‡åŒ–æ˜¯å¦ç”Ÿæ•ˆï¼Ÿ
**A**:
1. æ£€æŸ¥æ—¥å¿—ä¸­çš„ `[DUQUANT][REPLACED]` ä¿¡æ¯
2. æ£€æŸ¥ pack ç›®å½•ä¸­çš„æ–‡ä»¶æ•°é‡
3. å¯ç”¨ `OPENPI_DUQUANT_DEBUG=1` æŸ¥çœ‹è¯¦ç»†æ—¥å¿—

### Q4: é‡åŒ–åå‡†ç¡®ç‡ä¸‹é™æ€ä¹ˆåŠï¼Ÿ
**A**:
1. å¢åŠ  `OPENPI_DUQUANT_CALIB_STEPS` (æ›´å¤š calibration æ­¥æ•°)
2. è°ƒæ•´ `OPENPI_DUQUANT_ACT_PCT` (æ›´ä¿å®ˆçš„æ¿€æ´»é‡åŒ–)
3. å¢å¤§ `OPENPI_DUQUANT_BLOCK` (æ›´å¤§çš„ block size)
4. è°ƒæ•´ `OPENPI_DUQUANT_LS` (lambda smooth å‚æ•°)

### Q5: OOM é”™è¯¯æ€ä¹ˆè§£å†³ï¼Ÿ
**A**:
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

---

## 8. æ€»ç»“

æ•´ä¸ªæµç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š

1. **è„šæœ¬å¯åŠ¨** â†’ è®¾ç½®ç¯å¢ƒå˜é‡
2. **ä¸»ç¨‹åº** â†’ åŠ è½½æ¨¡å‹å’Œé…ç½®
3. **æ¨¡å‹åŠ è½½** â†’ ä» safetensors æ¢å¤æƒé‡
4. **DuQuant åˆå§‹åŒ–** â†’ æ›¿æ¢ Linear ä¸º DuQuantLinear
5. **Pack åŠ è½½/è®¡ç®—** â†’ åŠ è½½æˆ–è®¡ç®—æ—‹è½¬çŸ©é˜µ
6. **LIBERO å¾ªç¯** â†’ éå†ä»»åŠ¡å’Œ episodes
7. **æ¨ç†** â†’ Policy.sample_actions()
8. **å‰å‘ä¼ æ’­** â†’ PI0Pytorch.forward()
9. **DuQuant å‰å‘** â†’ DuQuantLinear.forward()
10. **åŠ¨ä½œæ‰§è¡Œ** â†’ ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œå¹¶è¯„ä¼°

å…³é”®ä¼˜åŒ–ï¼š
- âœ… é¢„è®¡ç®—æ—‹è½¬çŸ©é˜µ (packing)
- âœ… æ‰¹é‡æ—‹è½¬ (block diagonal)
- âœ… æƒé‡é¢„é‡åŒ–ç¼“å­˜
- âœ… æ¿€æ´» calibration
- âœ… CUDA å†…å­˜ä¼˜åŒ–

é€šè¿‡è¿™äº›ä¼˜åŒ–ï¼ŒDuQuant å¯ä»¥åœ¨å‡ ä¹ä¸é™ä½å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå®ç° 2-4x çš„å†…å­˜å‹ç¼©å’Œæ¥è¿‘åŸå§‹é€Ÿåº¦çš„æ¨ç†æ€§èƒ½ã€‚
