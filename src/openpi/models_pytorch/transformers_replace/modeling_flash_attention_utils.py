"""Shim module for flash attention kwargs compatibility."""

from transformers.modeling_flash_attention_utils import FlashAttentionKwargs  # noqa: F401

