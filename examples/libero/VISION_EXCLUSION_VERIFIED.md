# Vision Tower Exclusion Verification

## ‚úÖ Verification Status: PASSED

Vision tower layers are **correctly excluded** from DuQuant quantization in all scripts.

## üîç What Was Verified

We verified that the `OPENPI_DUQUANT_EXCLUDE` regex correctly excludes:

1. ‚úÖ **Vision Tower (SigLIP)**: `paligemma_with_expert.paligemma.model.vision_tower.*`
2. ‚úÖ **Multi-Modal Projector**: `paligemma_with_expert.paligemma.model.multi_modal_projector.*`
3. ‚úÖ **Embeddings**: `*.embed_tokens`
4. ‚úÖ **LM Head**: `*.lm_head`
5. ‚úÖ **Normalization Layers**: `*.layernorm`, `*_layernorm`

While **correctly including**:

1. ‚úÖ **LLM Attention**: `paligemma_with_expert.paligemma.model.language_model.layers[*].self_attn.{q,k,v,o}_proj`
2. ‚úÖ **LLM MLP**: `paligemma_with_expert.paligemma.model.language_model.layers[*].mlp.{gate,up,down}_proj`
3. ‚úÖ **DiT Attention**: `paligemma_with_expert.gemma_expert.model.layers[*].self_attn.{q,k,v,o}_proj`
4. ‚úÖ **DiT MLP**: `paligemma_with_expert.gemma_expert.model.layers[*].mlp.{gate,up,down}_proj`

## üìù Exclude Regex Used

```bash
export OPENPI_DUQUANT_EXCLUDE='(?:^|\.)(norm|ln|layernorm|emb|embed|vision_tower|vision|multi_modal_projector|lm_head)(?:\.|$)'
```

### How It Works

- `(?:^|\.)` - Matches start of string or a dot
- `(norm|ln|...)` - Matches any of the excluded keywords
- `(?:\.|$)` - Matches a dot or end of string

This ensures we match complete path components, not substrings.

### Example Matches

| Layer Path | Excluded? | Reason |
|------------|-----------|--------|
| `...vision_tower.encoder.layers.0.self_attn.q_proj` | ‚úÖ Yes | Contains `.vision_tower.` |
| `...multi_modal_projector.linear` | ‚úÖ Yes | Contains `.multi_modal_projector.` |
| `...language_model.layers.0.self_attn.q_proj` | ‚ùå No | No excluded keywords |
| `...gemma_expert.model.layers.0.mlp.gate_proj` | ‚ùå No | No excluded keywords |
| `...embed_tokens` | ‚úÖ Yes | Contains `.emb` |
| `...lm_head` | ‚úÖ Yes | Contains `.lm_head` |

## üß™ Test Script

We created a comprehensive test script to verify the exclusion:

```bash
python3 test_vision_exclusion.py
```

**Output:**
```
‚úÖ ALL TESTS PASSED!
‚úÖ Vision layers are correctly excluded
‚úÖ LLM and DiT layers are correctly included

üìä Summary:
  Vision layers excluded: 2/2
  LLM layers included: 3/3
  DiT layers included: 2/2
```

## üìä Model Structure

```
paligemma_with_expert (PaliGemmaWithExpertModel)
‚îú‚îÄ‚îÄ paligemma (PaliGemmaForConditionalGeneration)
‚îÇ   ‚îî‚îÄ‚îÄ model (PaliGemmaModel)
‚îÇ       ‚îú‚îÄ‚îÄ vision_tower (SigLIP)           ‚Üê ‚ùå EXCLUDED
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ encoder.layers[0-26]
‚îÇ       ‚îú‚îÄ‚îÄ multi_modal_projector           ‚Üê ‚ùå EXCLUDED
‚îÇ       ‚îî‚îÄ‚îÄ language_model (Gemma LLM)      ‚Üê ‚úÖ INCLUDED
‚îÇ           ‚îî‚îÄ‚îÄ layers[0-17]
‚îÇ               ‚îú‚îÄ‚îÄ self_attn.{q,k,v,o}_proj
‚îÇ               ‚îî‚îÄ‚îÄ mlp.{gate,up,down}_proj
‚îî‚îÄ‚îÄ gemma_expert (GemmaForCausalLM - DiT)   ‚Üê ‚úÖ INCLUDED
    ‚îî‚îÄ‚îÄ model.layers[0-17]
        ‚îú‚îÄ‚îÄ self_attn.{q,k,v,o}_proj
        ‚îî‚îÄ‚îÄ mlp.{gate,up,down}_proj
```

## üéØ Expected Layer Counts

### Full LLM+DiT Quantization (`run_full_llm_dit_w4a8.sh`)

| Component | Layers | Calculation |
|-----------|--------|-------------|
| Vision (SigLIP) | **0** | Excluded |
| Multi-Modal Projector | **0** | Excluded |
| LLM Attention | 72 | 18 layers √ó 4 projections |
| LLM MLP | 54 | 18 layers √ó 3 projections |
| DiT Attention | 72 | 18 layers √ó 4 projections |
| DiT MLP | 54 | 18 layers √ó 3 projections |
| **TOTAL** | **252** | LLM (126) + DiT (126) |

## ‚úÖ Scripts Updated

The following scripts have been updated with the correct exclusion regex:

1. ‚úÖ [`run_full_llm_dit_w4a8.sh`](run_full_llm_dit_w4a8.sh) - Line 68
2. ‚úÖ [`verify_duquant_layers.sh`](verify_duquant_layers.sh) - Line 100

## üöÄ How to Verify Yourself

### Option 1: Run the Test Script
```bash
cd /home/jz97/VLM_REPO/openpi
python3 test_vision_exclusion.py
```

### Option 2: Dry-Run Verification
```bash
cd /home/jz97/VLM_REPO/openpi/examples/libero
bash verify_duquant_layers.sh
```

This will show you exactly which layers will be quantized without actually running quantization.

### Option 3: Check Logs During Actual Run
```bash
bash run_full_llm_dit_w4a8.sh 2>&1 | grep "DUQUANT.*REPLACED"
```

Look for layer names in the output - you should **NOT** see any `vision_tower` layers.

## üêõ If Vision Layers Are Being Quantized

If you see vision layers being quantized, check:

1. **Environment variable is exported**:
   ```bash
   echo $OPENPI_DUQUANT_EXCLUDE
   ```

2. **No typos in the regex**:
   ```bash
   # Should contain: vision_tower|vision|multi_modal_projector
   ```

3. **Script is sourced correctly**:
   ```bash
   source examples/libero/.venv/bin/activate
   ```

## üìö Related Files

- Test script: [`test_vision_exclusion.py`](../../test_vision_exclusion.py)
- Full quantization: [`run_full_llm_dit_w4a8.sh`](run_full_llm_dit_w4a8.sh)
- Verification script: [`verify_duquant_layers.sh`](verify_duquant_layers.sh)
- DuQuant implementation: [`src/openpi/models_pytorch/duquant_layers.py`](../../src/openpi/models_pytorch/duquant_layers.py)

## üéâ Conclusion

The vision tower exclusion is **working correctly**. You can safely run full LLM+DiT quantization without worrying about quantizing the vision encoder.

The regex pattern correctly:
- ‚úÖ Excludes all vision_tower layers (SigLIP)
- ‚úÖ Excludes multi_modal_projector
- ‚úÖ Includes all LLM layers (126 layers)
- ‚úÖ Includes all DiT layers (126 layers)
- ‚úÖ Total: 252 layers quantized

---

*Last verified: 2025-10-11*
*Test status: ‚úÖ ALL TESTS PASSED*
